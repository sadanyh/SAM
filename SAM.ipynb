{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"I9hXkXtEg0T2","executionInfo":{"status":"ok","timestamp":1664207802709,"user_tz":-60,"elapsed":3,"user":{"displayName":"Hadeel Saadany","userId":"04760944246921259614"}}},"outputs":[],"source":["import nltk\n","from nltk.stem.porter import PorterStemmer\n","from nltk.corpus import wordnet\n","from itertools import chain, product\n","from nltk.corpus import sentiwordnet as swn\n","from nltk.stem import WordNetLemmatizer\n","from nltk import sent_tokenize, word_tokenize, pos_tag\n","import pandas as pd\n","import re\n","import time\n","from nltk.tag import pos_tag,map_tag\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import WordNetLemmatizer as form_replacer\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","import pandas as pd\n","import numpy as np\n","import spacy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y648DbTPg0UE"},"outputs":[],"source":["import nltk\n","nltk.download('universal_tagset')"]},{"cell_type":"markdown","metadata":{"id":"KcfLwyUag0UF"},"source":["# Sentiment Aware Metric (SAM)\n","\n","Calculates the sentiment scores of the mismatched words between hyp and ref. It computes the absoluted difference between the sum of the sentiment scores of mismatched words in hyp and ref"]},{"cell_type":"markdown","metadata":{"id":"Gu5kQ2Z-g0UI"},"source":["# Steps\n","1. Calculate the exact match between hyp and ref\n","2. Caculate the Sentiment Distance between the unmatched words according to Sentiment Words Lexicon \"prior sentiment scores\":\n","    1. decontract negative (e.g \"don't\" to 'do not')\n","    2. Assigns pos tags by Spacy, get the dependency relation\n","    3. Lemmatize with WordNet lemmatizer\n","    4. Change the lemmas into the SW lexicon entry form lemma#pos\n","    5. returns an enumerated list of lemma#pos (exclude the lemmas that do not have a POS in SW and hence zero sentiment score)\n","    6. change determiners ('DET') such as 'no, and conjunctions such as 'or' 'CCONJ' to Noun\n","    8. Compute the avergage between the scores of the mismatched words in hyp and ref\n","### NOTE:\n","The sentiment value of 'not' is increased to reflect a proportional penalty to the flipping of the sentiment"]},{"cell_type":"markdown","metadata":{"id":"no2lOm4Yg0UK"},"source":["# Calculate Exact Match"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpKZYl7bg0UM"},"outputs":[],"source":["def _generate_enums(hypothesis, reference, preprocess=str.lower): # this tokenizes and returnd (index,w) tuple\n","    \"\"\"\n","    Takes in string inputs for hypothesis and reference and returns\n","    enumerated word lists for each of them\n","\n","    :param hypothesis: hypothesis string\n","    :type hypothesis: str\n","    :param reference: reference string\n","    :type reference: str\n","    :preprocess: preprocessing method (default str.lower)\n","    :type preprocess: method\n","    :return: enumerated words list\n","    :rtype: list of 2D tuples, list of 2D tuples\n","    \"\"\"\n","    hypothesis_list = list(enumerate(preprocess(hypothesis).split()))\n","    reference_list = list(enumerate(preprocess(reference).split()))\n","    return hypothesis_list, reference_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GTn4Y8Fjg0UO"},"outputs":[],"source":["def exact_match(hypothesis, reference): # this combines the above and below functions together\n","    \"\"\"\n","    matches exact words in hypothesis and reference\n","    and returns a word mapping based on the enumerated\n","    word id between hypothesis and reference\n","\n","    :param hypothesis: hypothesis string\n","    :type hypothesis: str\n","    :param reference: reference string\n","    :type reference: str\n","    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n","             enumerated unmatched reference tuples\n","    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n","    \"\"\"\n","    hypothesis_list, reference_list = _generate_enums(hypothesis, reference)\n","    return _match_enums(hypothesis_list, reference_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dk9s7OKGg0UP"},"outputs":[],"source":["# it takes the (index,w) lists and makes a new tuple list of (indexfromhyp,indexfromref) for all the same words\n","# and returns what is left unmatched in as (indexhyp, w) (indexref,w)\n","def _match_enums(enum_hypothesis_list, enum_reference_list):\n","    \"\"\"\n","    matches exact words in hypothesis and reference and returns\n","    a word mapping between enum_hypothesis_list and enum_reference_list\n","    based on the enumerated word id.\n","\n","    :param enum_hypothesis_list: enumerated hypothesis list\n","    :type enum_hypothesis_list: list of tuples\n","    :param enum_reference_list: enumerated reference list\n","    :type enum_reference_list: list of 2D tuples\n","    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n","             enumerated unmatched reference tuples\n","    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n","    \"\"\"\n","    word_match = []\n","    for i in range(len(enum_hypothesis_list))[::-1]:\n","        for j in range(len(enum_reference_list))[::-1]:\n","            if enum_hypothesis_list[i][1] == enum_reference_list[j][1]:\n","                word_match.append( # attach the indeces without the words\n","                    (enum_hypothesis_list[i][0], enum_reference_list[j][0])\n","                )\n","                (enum_hypothesis_list.pop(i)[1], enum_reference_list.pop(j)[1]) # take out the matched tuple from the original list\n","                break\n","    return word_match, enum_hypothesis_list, enum_reference_list"]},{"cell_type":"markdown","metadata":{"id":"S-DZyfkPg0UQ"},"source":["# Calculate Sentiment Distance between the Unmached words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n5G2Cniyg0UR"},"outputs":[],"source":["# Function to decontract negatives and other abbr\n","\n","def decontracted(phrase):\n","    \n","    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n","    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n","    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n","    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n","    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n","    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n","    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n","    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n","    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n","    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n","    \n","    return phrase\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LrVyLKixg0UR"},"outputs":[],"source":["#function to turn the pos tags (for Spacy, NLTK pos taggin or Penn-tree) to WN pos tags to be compatible with SW lexicon\n","# turning Determiner and conjunction to noun to get the scores form SW\n","from nltk.corpus import wordnet as wn\n","\n","def is_noun(tag):\n","    return tag in ['NOUN', 'CCONJ', 'DET','NN', 'NNS', 'NNP', 'NNPS']\n","\n","\n","def is_verb(tag):\n","    return tag in ['VERB','VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n","\n","\n","def is_adverb(tag):\n","    return tag in ['ADV','RB', 'RBR', 'RBS']\n","\n","\n","def is_adjective(tag):\n","    return tag in ['ADJ','JJ', 'JJR', 'JJS']\n","\n","\n","def penn_to_wn(tag):\n","    if is_adjective(tag):\n","        return wn.ADJ\n","    elif is_noun(tag):\n","        return wn.NOUN\n","    elif is_adverb(tag):\n","        return wn.ADV\n","    elif is_verb(tag):\n","        return wn.VERB\n","    return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GvxG91w9g0US"},"outputs":[],"source":["# function that takes the raw hyp/ref and:\n","# 1. decontract negative (e.g \"don't\" to 'do not')\n","# 2. Assigns pos tags by Spacy, get the dependency relation\n","# 3. Lemmatize with WornNet lemmatizer\n","# 4. Get the lemmas in the SW lexicon entry form lemma#pos\n","# 5. returns an enumerated list of lemma#pos (excludes words that do not have lemmas in SW, 0 sentiment score)\n","\n","\n","\n","def preprocess2(tran):\n","    tokens = []\n","    \n","    decontr = decontracted(tran)\n","    nlp = spacy.load(\"en_core_web_sm\")\n","    doc = nlp(decontr.lower())\n","    spacy_ls = [(token.text, token.pos_, token.dep_) for token in doc]\n","    for i in range(len(spacy_ls)):\n","        try:\n","            t = form_replacer().lemmatize(spacy_ls[i][0], penn_to_wn(spacy_ls[i][1]))+\"#\"+ penn_to_wn(spacy_ls[i][1])  \n","        except:\n","            continue\n","        tokens.append(t)\n","    enum_tokens = list(enumerate(tokens))\n","    \n","    return enum_tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Htd74dsIg0UT","outputId":"dcb0a629-89b3-4b0d-9851-2aecdbc0b966"},"outputs":[{"data":{"text/plain":["[(0, 'the#n'),\n"," (1, 'sad#a'),\n"," (2, 'twitter#n'),\n"," (3, 'bring#v'),\n"," (4, 'really#r'),\n"," (5, 'depression#n')]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["preprocess2(h)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZjyuUsblg0UT"},"outputs":[],"source":["# function to get exact matches indexes, and mismatched hyp and ref words with POS tags\n","\n","def senti_exact_match(hyp,ref):\n","    hyp_ls = preprocess2(hyp)\n","    ref_ls = preprocess2(ref)\n","    word_match = []\n","    for i in range(len(hyp_ls))[::-1]:\n","        for j in range(len(ref_ls))[::-1]:\n","            if hyp_ls[i][1] == ref_ls[j][1]:\n","                word_match.append(\n","                    (hyp_ls[i][0], ref_ls[j][0])\n","                )\n","                (hyp_ls.pop(i)[1], ref_ls.pop(j)[1])\n","                break\n","    return word_match, hyp_ls, ref_ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JrEjcY7tg0UU"},"outputs":[],"source":["h = \"the sad Twitter brings really depression\" # the preprocess2 fixes the problem with brings\n","f = \" the sad twitter doesn't really bring happiness\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GcdF9Auwg0UU","outputId":"09dc9fea-6ef1-4c6a-d387-807281ff73af"},"outputs":[{"data":{"text/plain":["([(4, 5), (3, 6), (2, 2), (1, 1), (0, 0)],\n"," [(5, 'depression#n')],\n"," [(3, 'do#v'), (4, 'not#r'), (7, 'happiness#n')])"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["senti_exact_match(h,f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ccsQ8nK4g0UV"},"outputs":[],"source":["# function to calculate the sentiment score difference between mismatched words\n","\n","def senti_score(hyp,ref):  \n","    matched_words, hyp_ls, ref_ls = senti_exact_match(hyp,ref)\n","    column_names = ['lemmaPOS','score']\n","    df = pd.read_csv(\"C:/Users/sadan/OneDrive/PhD/metric/metrics/senti_lexicon/SentiWords.txt\", header=None, delimiter=\"\\t\", names=column_names)\n","    senti_dict = dict(df.values.tolist())\n","    hyp_values = []\n","    ref_values = []\n","    for key in senti_dict.keys():\n","        for i in range(len(hyp_ls)):      \n","            if  key == hyp_ls[i][1]: \n","                hyp_values.append(senti_dict[key])\n","        for j in range(len(ref_ls)):\n","            if  key == ref_ls[j][1]: \n","                ref_values.append(senti_dict[key])\n","    diff = abs(sum(hyp_values)-sum(ref_values))\n","    return diff, hyp_ls, ref_ls,hyp_values,ref_values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BBzOSECAg0UV"},"outputs":[],"source":["# function to calculate the sentiment score difference between mismatched words\n","\n","def senti_score2(hyp,ref):  \n","    matched_words, hyp_ls, ref_ls = senti_exact_match(hyp,ref)\n","    column_names = ['lemmaPOS','score']\n","    df = pd.read_csv(\"C:/Users/sadan/OneDrive/PhD/metric/metrics/senti_lexicon/SentiWords.txt\", header=None, delimiter=\"\\t\", names=column_names)\n","    senti_dict = dict(df.values.tolist())\n","    hyp_values = []\n","    ref_values = []\n","    for key in senti_dict.keys():\n","        for i in range(len(hyp_ls)):      \n","            if  key == hyp_ls[i][1]: \n","                hyp_values.append(senti_dict[key])\n","        for j in range(len(ref_ls)):\n","            if  key == ref_ls[j][1]: \n","                ref_values.append(senti_dict[key])\n","    #diff = abs(sum(hyp_values)-sum(ref_values))\n","    return {'trans_mismatch_num': len(hyp_ls), \n","            'ref_mismatch_num': len(ref_ls),\n","            'trans_senti_score':sum(hyp_values),\n","            'ref_senti_score': sum(ref_values),\n","            'num_matched_words' : len(matched_words)}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IESGHbQCg0UW"},"outputs":[],"source":["# function to calculate the sentiment score difference between mismatched words (take zero out)\n","\n","def senti_score3(hyp,ref):  \n","    matched_words, hyp_ls, ref_ls = senti_exact_match(hyp,ref)\n","    column_names = ['lemmaPOS','score']\n","    df = pd.read_csv(\"C:/Users/sadan/OneDrive/PhD/metric/metrics/senti_lexicon/SentiWords.txt\", header=None, delimiter=\"\\t\", names=column_names)\n","    senti_dict = dict(df.values.tolist())\n","    hyp_values = []\n","    ref_values = []\n","    for key in senti_dict.keys():\n","        try:\n","            for i in range(len(hyp_ls)):      \n","                if  key == hyp_ls[i][1]: \n","                    hyp_values.append(round(senti_dict[key],2))\n","        except:\n","            continue\n","        try:\n","            for j in range(len(ref_ls)):\n","                if  key == ref_ls[j][1]: \n","                    ref_values.append(round(senti_dict[key],2))\n","        except:\n","            continue\n","    \n","    diff = abs(sum(ref_values)-sum(hyp_values))\n","    return {'trans_mismatch': hyp_ls,\n","            'trans_senti_scores': hyp_values,\n","            'transi_mis_len' : len(hyp_ls),\n","            'ref_mismatch': ref_ls,            \n","            'ref_senti_scores': ref_values,\n","            'ref_mis_len' : len(ref_ls),\n","            'absulte_Diff': round(diff,2),\n","            'num_matched_words' : len(matched_words)}\n","            "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c-ksXLKbg0UW"},"outputs":[],"source":["# function to calculate the sentiment score difference between mismatched words (take zero out)\n","\n","def senti_score4(hyp,ref):  \n","    matched_words, hyp_ls, ref_ls = senti_exact_match(hyp,ref)\n","    column_names = ['lemmaPOS','score']\n","    df = pd.read_csv(\"C:/Users/sadan/OneDrive/PhD/metric/metrics/senti_lexicon/SentiWords.txt\", header=None, delimiter=\"\\t\", names=column_names)\n","    senti_dict = dict(df.values.tolist())\n","    hyp_values = []\n","    ref_values = []\n","    for key in senti_dict.keys():\n","        for i in range(len(hyp_ls)):      \n","            if  key == hyp_ls[i][1]: \n","                hyp_values.append(round(senti_dict[key],2))\n","        for j in range(len(ref_ls)):\n","            if  key == ref_ls[j][1]: \n","                ref_values.append(round(senti_dict[key],2))\n","    \n","    #diff = abs(sum(hyp_values)-sum(ref_values))\n","    return hyp_values, ref_values"]},{"cell_type":"markdown","metadata":{"id":"MakzHtLKg0UX"},"source":["# Assigning penalty"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"higOW8Jxg0UX"},"outputs":[],"source":["import numpy\n","def senti_penalty(ls):\n","    result = numpy.dot( list(map(abs, ls)),ls)/sum(list(map(abs, ls)))\n","    return result\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python [conda env:old_env]","language":"python","name":"conda-env-old_env-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}